It will work with a local webpage ran 0n 127.0.0.1. It will provide control over all the stored files, vectorised files, and local AI models. It will provide access and download to reputable AI models. When going online it starts a new server running on ngrok. users must configure and provied a public key them selves which they get from ngrok web page themselves. When going online a API running on the ngrok server will be given to the node through a pre strored address. When going online they will be reuired to log in aswell. They will be given a session token that lasts for a certain optimised duration. The API will store the endpoint for the delta time of the token. When a certain tab is selected on the a request is sent to a certain url ofthe api to recieve the data. The AI model tab will load all the AI models hosted locally on other people's computers. People can also view which files have been uploaded for the RAG capability og the locally hosted llm. A seperate tab is used for downloading data sets for them to use it on their locally ran AI model. Only data sets which have been embedded for the specific local llm they are running is shown.

The unique and core motive to this is users can also request to embed chunks of documents on other peoples devices who are connected. When going online the API creates sends all the active endpoints to the user to be stored locally. Then the local server sends requests to all the endpoints in a predefined manner to see if they are accesible. Those who give the a response are then stored in an area that the computer will access when creating a document processing request. The computer then submits chunks of text to the ngrok endpoints that are working and they will recieve it through their ngrok endpoint. It then checks if all chunks of text are recieved before storing it. There will also have to be a saved data of endpoints storing the computing capabilities of the other endpoints to ensure bottlenecks do not occur. This ensures all embeddings will be recieved in a specific amount of time. If not recieved the local server will close the recieving endpoint and process the embedings of chunks of text whose embeddings where not received. Once all the embedding ID's are there then it will save it to a chromadb database for use of models. 

Users will also have local endpoints which shares stored databases. The text correlated with the embeddings database will be shown when accessing the specific endpoint and the then their local llm's can use the embddings database located on a specific ngrok endpoint. 

